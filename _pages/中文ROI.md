正如我们所知道的，神经网络不像线性回归那样简单，复杂而又深层的卷积和跳跃连接使得结构让人琢磨不透（不像是decision tree，每个节点上的分划都是可解释的）。在基于梯度下降的神经网络之前，是人工设计一个具有语义的卷积核，例如索贝尔算子，罗伯特交叉梯度算子，这些都是用于特定任务（边缘检测）的算子。在这些算子的背后，是人用智慧手工设计出来的。而当卷积神经网络问世以后，这些自主学习的多层的卷积核，便成为了未解之谜。－－他们到底学到了什么？
